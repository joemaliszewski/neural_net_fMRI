{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework for MRI reconstruction (Autumn 2019)\n",
    "\n",
    "In this tutorial, we provide the data loader to read and process the MRI data in order to ease the difficulty of training your network. By providing this, we hope you focus more on methodology development. Please feel free to change it to suit what you need."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Introduction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL IMPORTS\n",
    "import h5py, os\n",
    "from functions import transforms as T\n",
    "from functions.subsample import MaskFunc\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "#cuda\n",
    "import torch.optim as optim\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu' #'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### UNET MODEL ######\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Convolutional Block that consists of two convolution layers each followed by\n",
    "    instance normalization, relu activation and dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, drop_prob):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input.\n",
    "            out_chans (int): Number of channels in the output.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(drop_prob),\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(drop_prob)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        return self.layers(input)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'ConvBlock(in_chans={self.in_chans}, out_chans={self.out_chans}, ' \\\n",
    "            f'drop_prob={self.drop_prob})'\n",
    "\n",
    "\n",
    "class UnetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of a U-Net model.\n",
    "\n",
    "    This is based on:\n",
    "        Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks\n",
    "        for biomedical image segmentation. In International Conference on Medical image\n",
    "        computing and computer-assisted intervention, pages 234â€“241. Springer, 2015.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, chans, num_pool_layers, drop_prob):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input to the U-Net model.\n",
    "            out_chans (int): Number of channels in the output to the U-Net model.\n",
    "            chans (int): Number of output channels of the first convolution layer.\n",
    "            num_pool_layers (int): Number of down-sampling and up-sampling layers.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.chans = chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.down_sample_layers = nn.ModuleList([ConvBlock(in_chans, chans, drop_prob)])\n",
    "        ch = chans\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.down_sample_layers += [ConvBlock(ch, ch * 2, drop_prob)]\n",
    "            ch *= 2\n",
    "        self.conv = ConvBlock(ch, ch, drop_prob)\n",
    "\n",
    "        self.up_sample_layers = nn.ModuleList()\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.up_sample_layers += [ConvBlock(ch * 2, ch // 2, drop_prob)]\n",
    "            ch //= 2\n",
    "        self.up_sample_layers += [ConvBlock(ch * 2, ch, drop_prob)]\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch // 2, kernel_size=1),\n",
    "            nn.Conv2d(ch // 2, out_chans, kernel_size=1),\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        stack = []\n",
    "        output = input\n",
    "        # Apply down-sampling layers\n",
    "        for layer in self.down_sample_layers:\n",
    "            output = layer(output)\n",
    "            stack.append(output)\n",
    "            output = F.max_pool2d(output, kernel_size=2)\n",
    "\n",
    "        output = self.conv(output)\n",
    "\n",
    "        # Apply up-sampling layers\n",
    "        for layer in self.up_sample_layers:\n",
    "            output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            output = torch.cat([output, stack.pop()], dim=1)\n",
    "            output = layer(output)\n",
    "        return self.conv2(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-defined ultily functions used in creating the neural net\n",
    "\n",
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')\n",
    "\n",
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed=True):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "\n",
    "    fname, rawdata_name, slice = subject_id  \n",
    "    \n",
    "    with h5py.File(rawdata_name, 'r') as data:\n",
    "        rawdata = data['kspace'][slice]\n",
    "                      \n",
    "    slice_kspace = T.to_tensor(rawdata).unsqueeze(0)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "\n",
    "    # apply random mask\n",
    "    shape = np.array(slice_kspace.shape)\n",
    "    mask_func = MaskFunc(center_fractions=[center_fract], accelerations=[acc])\n",
    "    seed = None if not use_seed else tuple(map(ord, fname))\n",
    "    mask = mask_func(shape, seed)\n",
    "      \n",
    "    # undersample\n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "    masks = mask.repeat(S, Ny, 1, ps)\n",
    "\n",
    "    img_gt, img_und = T.ifft2(slice_kspace), T.ifft2(masked_kspace)\n",
    "\n",
    "    # perform data normalization which is important for network to learn useful features\n",
    "    # during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "    norm = T.complex_abs(img_und).max()\n",
    "    if norm < 1e-6: norm = 1e-6\n",
    "    \n",
    "#     A = cropped_img_und.squeeze().detach().cpu()\n",
    "#     C = cropped_gt.squeeze().squeeze().detach().cpu()\n",
    "    \n",
    "    # normalized data\n",
    "    img_gt, img_und, rawdata_und = img_gt/norm, img_und/norm, masked_kspace/norm\n",
    "    \n",
    "    volume_image_abs = T.complex_abs(img_gt) \n",
    "    cropped_gt = T.center_crop(volume_image_abs, [320, 320])\n",
    "    cropped_gt = cropped_gt\n",
    "        \n",
    "    volume_image_abs = T.complex_abs(img_und) \n",
    "    cropped_img_und = T.center_crop(volume_image_abs, [320, 320])\n",
    "    cropped_img_und = cropped_img_und\n",
    "        \n",
    "    return cropped_gt,cropped_img_und, norm\n",
    "  #return img_gt.squeeze(0), img_und.squeeze(0), rawdata_und.squeeze(0), masks.squeeze(0), norm\n",
    "\n",
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)\n",
    "    \n",
    "def load_data_path(train_data_path, val_data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "\n",
    "    data_list = {}\n",
    "    train_and_val = ['train', 'val']\n",
    "    data_path = [train_data_path, val_data_path]\n",
    "\n",
    "    for i in range(len(data_path)):\n",
    "\n",
    "        data_list[train_and_val[i]] = []\n",
    "\n",
    "        which_data_path = data_path[i]\n",
    "\n",
    "        for fname in sorted(os.listdir(which_data_path)):\n",
    "\n",
    "            subject_data_path = os.path.join(which_data_path, fname)\n",
    "\n",
    "            if not os.path.isfile(subject_data_path): continue \n",
    "\n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                num_slice = data['kspace'].shape[0]\n",
    "\n",
    "            # the first 5 slices are mostly noise so it is better to exlude them\n",
    "            data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(5, num_slice)]\n",
    "\n",
    "    return data_list  \n",
    "   \n",
    "    \n",
    "from skimage.measure import compare_ssim  \n",
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "    return compare_ssim(\n",
    "        gt.transpose(1, 2, 0), pred.transpose(1, 2, 0), multichannel=True, data_range=gt.max()\n",
    "    )  \n",
    "\n",
    "\n",
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Now we can create a model\n",
    "model = UnetModel(\n",
    "    in_chans = 1,\n",
    "    out_chans = 1,\n",
    "    chans = 32,\n",
    "    num_pool_layers = 4,\n",
    "    drop_prob = 0 ).to(device) # FirstModel().to(device)\n",
    "\n",
    "# # we can also inspect its parameters\n",
    "# print(\"Before training: \\n\", model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Set initial path for the fullset of training data\n",
    "# file_path = '/data/local/NC2019MRI/train'\n",
    "\n",
    "# # Shuffle sets, and split train_index into train and validation 80:20 respectively \n",
    "# numfiles_all = len(os.listdir(file_path))\n",
    "# print (numfiles_all)\n",
    "# indx = np.arange(numfiles_all)\n",
    "# np.random.shuffle(indx)\n",
    "\n",
    "# split_80_20 = 0.8 * numfiles_all\n",
    "# split_80_20 = int(split_80_20)\n",
    "# print(split_80_20)\n",
    "\n",
    "# np.random.seed(42)\n",
    "\n",
    "# train_inx = indx[:split_80_20]\n",
    "# val_inx = indx[split_80_20:]\n",
    "\n",
    "# print(\"train_inx\",train_inx)\n",
    "# print(\"val_inx\",val_inx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     train_data_list = {}\n",
    "#     train_data_list['val']= [0] * len(val_inx)\n",
    "#     train_data_list['train'] = [0] * len(train_inx)\n",
    "#     count = 0  \n",
    "    \n",
    "#     for fname in sorted(os.listdir(file_path)): \n",
    "#         subject_path = os.path.join(file_path, fname)\n",
    "#         with h5py.File(subject_path,  \"r\") as hf:\n",
    "#              total_num_slices = hf['kspace'].shape[0]\n",
    "\n",
    "                \n",
    "#         for i in range(len(train_inx)):\n",
    "#             if (train_inx[i] == count):\n",
    "#                 train_data_list['train'][i] = (file_path+ \"/\" +fname, fname, total_num_slices)  \n",
    "#                 break\n",
    "                \n",
    "#         for k in range(len(val_inx)):\n",
    "#             if (val_inx[k] == count):\n",
    "#                 train_data_list['val'][k] = (file_path+ \"/\"+ fname, fname, total_num_slices)\n",
    "#                 break\n",
    "#         count = count +1\n",
    "\n",
    "            \n",
    "#     train_data_final ={}         \n",
    "#     train_data_final['train'] = []  \n",
    "#     train_data_final['val'] = []  \n",
    "           \n",
    "#     for i in range (len(train_data_list['train'])):\n",
    "#         for slice in range (5,train_data_list['train'][i][2]):\n",
    "#             train_data_final['train'].append((train_data_list['train'][i][1], train_data_list['train'][i][0], (slice)))\n",
    "        \n",
    "\n",
    "#     for i in range (len(train_data_list['val'])):\n",
    "#         for slice in range (5,train_data_list['val'][i][2]):\n",
    "#             train_data_final['val'].append((train_data_list['val'][i][1], train_data_list['val'][i][0], slice))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"VALIDATION SET\")\n",
    "# valcount = 0\n",
    "# for i in range (len(train_data_final['val'])):\n",
    "#     print(\" count : \",valcount,\"  \", train_data_final['val'][i])\n",
    "#     valcount = valcount + 1\n",
    "    \n",
    "# print(\"\")\n",
    "# print(\"\")\n",
    "# print(\"\")\n",
    "# print(\"\")\n",
    "\n",
    "# train_count = 0    \n",
    "# print(\"TRAIN SET\")\n",
    "# for i in range (len(train_data_final['train'])):\n",
    "#     print(\" count : \",train_count,\"  \", train_data_final['train'][i])\n",
    "#     train_count = train_count + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    data_path_train = '/data/local/NC2019MRI/train'\n",
    "    data_path_val = '/data/local/NC2019MRI/train'\n",
    "    data_list = load_data_path(data_path_train, data_path_val) # first load all file names, paths and slices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "######loads_train_data #######\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    acc = 4\n",
    "    cen_fract = 0.08\n",
    "    seed = False # random masks for each slice \n",
    "    num_workers = 12 \n",
    "    \n",
    "    \n",
    "    #\n",
    "    train_dataset = MRIDataset(data_list['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=5, num_workers=num_workers) \n",
    "    \n",
    "#     validation_dataset = MRIDataset(train_data_final['val'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "#     validation_loader = DataLoader(validation_dataset, shuffle=True, batch_size=5, num_workers=num_workers)  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The Unet model below created by MIT, and was used in the assignment as a platform to build the neural network. https://github.com/facebookresearch/fastMRI.git"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "During the iterative cycle of training, the model may begin to overfit the model, rendering it ineffective to \n",
    "\n",
    "The a validation set is created in order to ensure the model is not \"over fitting\" the training data. An "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELOLOOO\n",
      "0 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bham/modules/roots/neural-comp/2019-20/lib/python3.6/site-packages/ipykernel_launcher.py:103: UserWarning: DEPRECATED: skimage.measure.compare_ssim has been moved to skimage.metrics.structural_similarity. It will be removed from skimage.measure in version 0.18.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "2 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "3 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "4 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "5 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "6 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "7 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "8 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "9 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "10 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "11 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "12 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "13 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "14 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "15 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "16 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "17 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "18 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "19 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "20 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "21 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "22 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "23 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "24 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "25 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "26 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "27 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "28 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "29 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "30 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "31 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "32 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "33 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "34 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "35 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "36 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "37 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "38 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "39 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "40 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "41 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "42 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "43 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "44 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "45 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "46 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "47 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "48 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "49 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "50 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "51 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "52 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "53 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "54 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "55 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "56 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "57 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "58 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "59 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "60 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "61 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "62 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "63 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "64 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "65 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "66 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "67 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n",
      "68 !!!!!size: torch.Size([5, 1, 320, 320]) torch.Size([5, 1, 320, 320])\n"
     ]
    }
   ],
   "source": [
    "#set training\n",
    "%matplotlib inline \n",
    "print(\"HELOLOOO\")\n",
    "\n",
    "# set learning rate\n",
    "#lr = 1e-4\n",
    "# lr = 1e-3\n",
    "# lr = 1e-2\n",
    "lr = 1e-1\n",
    "\n",
    "#set number of epoches, i.e., number of times we iterate through the training set\n",
    "epoches = 50\n",
    "\n",
    "# # We use mean square error (MSELoss)\n",
    "loss_fn = nn.MSELoss(reduction='mean').to(device)\n",
    "\n",
    "# Optimisers\n",
    "#optimiser = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "#optimiser = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Lists for plotting val/train SSIM/Loss/prEpoch\n",
    "train_av_epoch_ssim_list = []\n",
    "train_av_epoch_loss_list = []\n",
    "\n",
    "val_av_epoch_ssim_list = []\n",
    "val_av_epoch_loss_list = []\n",
    "\n",
    "epoch_count = 0\n",
    "\n",
    "#Model training\n",
    "for epoch in range(epoches):\n",
    "   \n",
    "    model.train()\n",
    "    \n",
    "    train_total_loss = 0\n",
    "    train_total_ssim = 0\n",
    "    train_iterations= 0\n",
    "    \n",
    "    val_total_loss = 0\n",
    "    val_total_ssim = 0\n",
    "    val_iterations= 0\n",
    "    \n",
    "    #Training\n",
    "    for data in train_loader:\n",
    "\n",
    "        # set the model to training mode\n",
    "        img_gt, img_und, _ = data\n",
    "        \n",
    "        #Reconstruction/prediction from undersampled data\n",
    "        img_gt = img_gt.to(device) \n",
    "        img_und.to(device) \n",
    "        optimiser.zero_grad()\n",
    "        print(train_iterations, '!!!!!size:',img_und.size(), img_gt.size())\n",
    "        y_pred = model(img_und.to(device))\n",
    "        \n",
    "                #visualise \n",
    "        # 0 = UNDERSAMPLED. 1 = PREDICTION. 2 = GROUND_TRUTH\n",
    "        #if train_iterations % 10 == 1:\n",
    "        A = img_und.squeeze().squeeze().numpy()[0:1,:,:]\n",
    "        B = y_pred.squeeze().squeeze().squeeze(0).detach().cpu()\n",
    "        \n",
    "        B = y_pred.squeeze().data.cpu().numpy()\n",
    "        B = B[0:1, :, :]\n",
    "        #print(B.shape)\n",
    "        \n",
    "        C = img_gt.squeeze().squeeze().squeeze(0).cpu().numpy()[0:1,:,:]\n",
    "        #print(\"A\",A.shape,\"B\",B.shape, \"C\",C.shape)\n",
    "        #all_imgs = torch.stack([A.squeeze(),B,C], dim=0)\n",
    "        #print(all_imgs.shape)\n",
    "        \n",
    "        all_imgs = np.concatenate([A,B,C], axis=0)\n",
    "        \n",
    "        #print(all_imgs.shape)\n",
    "        \n",
    "        #show_slices(all_imgs, [0,1,2], cmap='gray')\n",
    "        #plt.pause(1)\n",
    "        #print(\"2\")\n",
    "        \n",
    "        #calculate the loss from training data\n",
    "        loss = loss_fn(img_gt.to(device), y_pred)\n",
    "        loss.backward() \n",
    "        optimiser.step()\n",
    "        \n",
    "        #calculate total loss for epoch to generate average loss\n",
    "        train_total_loss = train_total_loss + loss.item()\n",
    "        \n",
    "        # calculate total ssim value for epoch to generate average ssim\n",
    "        ssim_value = ssim(C,B)\n",
    "        train_total_ssim = train_total_ssim + ssim_value\n",
    "     \n",
    "\n",
    "        #count\n",
    "        train_iterations = train_iterations +1\n",
    "        \n",
    "        \n",
    "    train_av_ssim = train_total_ssim/train_iterations\n",
    "    train_av_loss = train_total_loss/train_iterations\n",
    "    \n",
    "    train_av_epoch_ssim_list.append(train_av_ssim)\n",
    "    train_av_epoch_loss_list.append(train_av_loss)\n",
    "    \n",
    "    \n",
    "#     #Validation \n",
    "#     with torch.no_grad():\n",
    "#         #set to evaluation mode\n",
    "#         model.eval()\n",
    "#         for data in validation_loader:\n",
    "\n",
    "#             #from data loader\n",
    "#             img_gt, img_und, _ = data\n",
    "            \n",
    "#             #Reconstruction/prediction from undersampled data\n",
    "#             img_gt = img_gt.to(device) \n",
    "#             img_und.to(device) \n",
    "#             optimiser.zero_grad()\n",
    "#             print('!!!!!size:',img_und.size(), img_gt.size())\n",
    "#             y_pred = model(img_und.to(device))\n",
    "            \n",
    "#             A = img_und.squeeze().squeeze().numpy()[0:1,:,:]\n",
    "#             B = y_pred.squeeze().squeeze().squeeze(0).detach().cpu()\n",
    "        \n",
    "#             B = y_pred.squeeze().data.cpu().numpy()\n",
    "#             B = B[0:1, :, :]\n",
    "            \n",
    "#         #print(B.shape)\n",
    "        \n",
    "#             C = img_gt.squeeze().squeeze().squeeze(0).cpu().numpy()[0:1,:,:]\n",
    "#             all_imgs = np.concatenate([A,B,C], axis=0)\n",
    "            \n",
    "            \n",
    "#              #calculate the loss from validation data\n",
    "#             val_loss = loss_fn(img_gt.to(device), y_pred)\n",
    "#             optimiser.step()\n",
    "            \n",
    "#             #calculate total loss for epoch to generate average loss\n",
    "#             val_total_loss = val_total_loss + loss.item()\n",
    "        \n",
    "        \n",
    "# #             A = img_und.unsqueeze(1).n\n",
    "# #             B = y_pred.squeeze().squeeze().detach().cpu().numpy()\n",
    "# #             C = img_gt.squeeze().squeeze().cpu().numpy()\n",
    "# #             #print(A.shape, B.shape, C.shape)\n",
    "            \n",
    "#             #all_imgs = torch.stack([A.squeeze(),B,C], dim=0)\n",
    "#             #print(\"1\")\n",
    "#            # show_slices(all_imgs, [0,1,2], cmap='gray')\n",
    "#             #plt.pause(1)\n",
    "#             #print(\"2\")\n",
    "#             #calculate total ssim value for epoch to generate average ssim\n",
    "#             ssim_value = ssim(C,B)\n",
    "#             val_total_ssim = val_total_ssim + ssim_value\n",
    "            \n",
    "#             #count\n",
    "#             val_iterations = val_iterations + 1\n",
    "            \n",
    "#     val_av_ssim = val_total_ssim/val_iterations\n",
    "#     val_av_loss = val_total_loss/val_iterations\n",
    "    \n",
    "#     val_av_epoch_ssim_list.append(val_av_ssim)\n",
    "#     val_av_epoch_loss_list.append(val_av_loss)\n",
    "    \n",
    "#     epoch_count = epoch_count + 1\n",
    "    print(\"epoch_count \", epoch_count)\n",
    "\n",
    "print(\"epoch: \", epoch_count,\"train_av_epoch_ssim_list :\", train_av_epoch_ssim_list)\n",
    "print(\"epoch: \", epoch_count,\"train_av_epoch_loss_list :\", train_av_epoch_loss_list)\n",
    "#print(\"epoch: \", epoch_count,\"val_av_epoch_ssim_list :\", val_av_epoch_ssim_list)\n",
    "#print(\"epoch: \", epoch_count,\"val_av_epoch_loss_list :\", val_av_epoch_loss_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALIDATION VS TRAINING SSIM COMPARISON\n",
    "plt.plot(range(len(train_av_epoch_ssim_list)), train_av_epoch_ssim_list,'r', label = \"Av Train SSIM\")\n",
    "plt.plot(range(len(val_av_epoch_ssim_list)), val_av_epoch_ssim_list,'b', label = \"Av Val SSIM\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Av SSIM\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALIDATION VS TRAINING LOSS COMPARISON\n",
    "plt.plot(range(len(train_av_epoch_loss_list)), train_av_epoch_loss_list,'r', label = \"Av Training loss\")\n",
    "plt.plot(range(len(val_av_epoch_loss_list)), val_av_epoch_loss_list,'b', label = \"Av Val loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"unet_model_save_50ep_adam_le1_b5_L1_4F.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_av_epoch_loss_list\n",
    "#x = val_av_epoch_loss_list\n",
    "\n",
    "#x = train_av_epoch_ssim_list\n",
    "#x = val_av_epoch_ssim_list\n",
    "\n",
    "total = 0\n",
    "for i in range(len(x)):\n",
    "    total = total +x[i]\n",
    "    av = total/(len(x))\n",
    "print(av)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
