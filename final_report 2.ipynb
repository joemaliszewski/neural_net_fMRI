{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework for MRI reconstruction (Autumn 2019) Group 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best reconstruction results : https://drive.google.com/drive/folders/1OGyvaKbVwCAailivJueuA3n5J_D18vbq?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Implmentation\n",
    "  \n",
    "Below shows the source code for the neural network that was implemented. Within the code, them performance anaylisis mechanisms were described when neccessary. Some of the features described in the implementation refer to the report, where in places it is described in more detail.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL IMPORTS\n",
    "import h5py, os\n",
    "from functions import transforms as T\n",
    "from functions.subsample import MaskFunc\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "#cuda\n",
    "import torch.optim as optim\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu' #'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The Unet model below created by MIT, and was used in this assignment as a platform to build the neural network. https://github.com/facebookresearch/fastMRI.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNET MODEL\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Convolutional Block that consists of two convolution layers each followed by\n",
    "    instance normalization, relu activation and dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, drop_prob):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input.\n",
    "            out_chans (int): Number of channels in the output.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(drop_prob),\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(drop_prob)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        return self.layers(input)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'ConvBlock(in_chans={self.in_chans}, out_chans={self.out_chans}, ' \\\n",
    "            f'drop_prob={self.drop_prob})'\n",
    "\n",
    "\n",
    "class UnetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of a U-Net model.\n",
    "\n",
    "    This is based on:\n",
    "        Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks\n",
    "        for biomedical image segmentation. In International Conference on Medical image\n",
    "        computing and computer-assisted intervention, pages 234–241. Springer, 2015.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, chans, num_pool_layers, drop_prob):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input to the U-Net model.\n",
    "            out_chans (int): Number of channels in the output to the U-Net model.\n",
    "            chans (int): Number of output channels of the first convolution layer.\n",
    "            num_pool_layers (int): Number of down-sampling and up-sampling layers.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.chans = chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.down_sample_layers = nn.ModuleList([ConvBlock(in_chans, chans, drop_prob)])\n",
    "        ch = chans\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.down_sample_layers += [ConvBlock(ch, ch * 2, drop_prob)]\n",
    "            ch *= 2\n",
    "        self.conv = ConvBlock(ch, ch, drop_prob)\n",
    "\n",
    "        self.up_sample_layers = nn.ModuleList()\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.up_sample_layers += [ConvBlock(ch * 2, ch // 2, drop_prob)]\n",
    "            ch //= 2\n",
    "        self.up_sample_layers += [ConvBlock(ch * 2, ch, drop_prob)]\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch // 2, kernel_size=1),\n",
    "            nn.Conv2d(ch // 2, out_chans, kernel_size=1),\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        stack = []\n",
    "        output = input\n",
    "        # Apply down-sampling layers\n",
    "        for layer in self.down_sample_layers:\n",
    "            output = layer(output)\n",
    "            stack.append(output)\n",
    "            output = F.max_pool2d(output, kernel_size=2)\n",
    "\n",
    "        output = self.conv(output)\n",
    "\n",
    "        # Apply up-sampling layers\n",
    "        for layer in self.up_sample_layers:\n",
    "            output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            output = torch.cat([output, stack.pop()], dim=1)\n",
    "            output = layer(output)\n",
    "        return self.conv2(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######This function was amended to allow for changes in the batch size.######\n",
    "\n",
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed=True):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "\n",
    "    fname, rawdata_name, slice = subject_id  \n",
    "    \n",
    "    with h5py.File(rawdata_name, 'r') as data:\n",
    "        rawdata = data['kspace'][slice]\n",
    "                      \n",
    "    slice_kspace = T.to_tensor(rawdata).unsqueeze(0)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "\n",
    "    # apply random mask\n",
    "    shape = np.array(slice_kspace.shape)\n",
    "    mask_func = MaskFunc(center_fractions=[center_fract], accelerations=[acc])\n",
    "    seed = None if not use_seed else tuple(map(ord, fname))\n",
    "    mask = mask_func(shape, seed)\n",
    "      \n",
    "    # undersample\n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "    masks = mask.repeat(S, Ny, 1, ps)\n",
    "\n",
    "    img_gt, img_und = T.ifft2(slice_kspace), T.ifft2(masked_kspace)\n",
    "\n",
    "    # perform data normalization which is important for network to learn useful features\n",
    "    # during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "    norm = T.complex_abs(img_und).max()\n",
    "    if norm < 1e-6: norm = 1e-6\n",
    "    \n",
    "    # normalized data\n",
    "    img_gt, img_und, rawdata_und = img_gt/norm, img_und/norm, masked_kspace/norm\n",
    "    \n",
    "    ######### AMENDMENT #########\n",
    "    \n",
    "    #convert complex numbers to abs to ensure it is useable.\n",
    "    #Cropping was completed here to allow for images to be \n",
    "    #compared effectively and can be placed over eachother 320 by 320 square\n",
    "    \n",
    "    volume_image_abs = T.complex_abs(img_gt) \n",
    "    cropped_gt = T.center_crop(volume_image_abs, [320, 320])\n",
    "    cropped_gt = cropped_gt\n",
    "        \n",
    "    volume_image_abs = T.complex_abs(img_und) \n",
    "    cropped_img_und = T.center_crop(volume_image_abs, [320, 320])\n",
    "    cropped_img_und = cropped_img_und\n",
    "        \n",
    "    return cropped_gt,cropped_img_und, norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-defined ultily functions used in creating the neural net\n",
    "\n",
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')\n",
    "\n",
    "\n",
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)\n",
    "    \n",
    "def load_data_path(train_data_path, val_data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "\n",
    "    data_list = {}\n",
    "    train_and_val = ['train', 'val']\n",
    "    data_path = [train_data_path, val_data_path]\n",
    "\n",
    "    for i in range(len(data_path)):\n",
    "\n",
    "        data_list[train_and_val[i]] = []\n",
    "\n",
    "        which_data_path = data_path[i]\n",
    "\n",
    "        for fname in sorted(os.listdir(which_data_path)):\n",
    "\n",
    "            subject_data_path = os.path.join(which_data_path, fname)\n",
    "\n",
    "            if not os.path.isfile(subject_data_path): continue \n",
    "\n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                num_slice = data['kspace'].shape[0]\n",
    "\n",
    "            # the first 5 slices are mostly noise so it is better to exlude them\n",
    "            data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(5, num_slice)]\n",
    "\n",
    "    return data_list  \n",
    "   \n",
    "    \n",
    "from skimage.measure import compare_ssim  \n",
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "    return compare_ssim(\n",
    "        gt.transpose(1, 2, 0), pred.transpose(1, 2, 0), multichannel=True, data_range=gt.max()\n",
    "    )  \n",
    "\n",
    "\n",
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-a940ee79c7ec>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-a940ee79c7ec>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Below is where the model is created. #### Describe each parameter.\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Below is where the model is created. #### Describe each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a model\n",
    "model = UnetModel(\n",
    "    in_chans = 1,\n",
    "    out_chans = 1,\n",
    "    chans = 32,\n",
    "    num_pool_layers = 4,\n",
    "    drop_prob = 0 ).to(device) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The data set is shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set initial path for the fullset of training data\n",
    "file_path = '/data/local/NC2019MRI/train'\n",
    "\n",
    "# Shuffle sets, and split train_index into train and validation 80:20 respectively. \n",
    "#this was chosen as a reasonable split for the data set size. Any less the validation results \n",
    "#may prove unrepresentative, whereas any more would deprive the model of useful information to train with\n",
    "\n",
    "#Shuffling was done to ensure higher probabilty that the neural model will be trained in such as way,\n",
    "# that it will generalize over unseen data sets.\n",
    "numfiles_all = len(os.listdir(file_path))\n",
    "print (numfiles_all)\n",
    "indx = np.arange(numfiles_all)\n",
    "np.random.shuffle(indx)\n",
    "\n",
    "split_80_20 = 0.8 * numfiles_all\n",
    "split_80_20 = int(split_80_20)\n",
    "print(split_80_20)\n",
    "\n",
    "#############\n",
    "np.random.seed(42)\n",
    "\n",
    "train_inx = indx[:split_80_20]\n",
    "val_inx = indx[split_80_20:]\n",
    "\n",
    "#varify split and shuffling\n",
    "print(\"train_inx\",train_inx)\n",
    "print(\"val_inx\",val_inx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    #set list for storage of both train and validation data\n",
    "    train_data_list = {}\n",
    "    train_data_list['val']= [0] * len(val_inx)\n",
    "    train_data_list['train'] = [0] * len(train_inx)\n",
    "    count = 0  \n",
    "    \n",
    "    #read in and store the h5 files from the data set in the coorect order as in the shuffed indexes\n",
    "    #for each set\n",
    "    for fname in sorted(os.listdir(file_path)): \n",
    "        subject_path = os.path.join(file_path, fname)\n",
    "        with h5py.File(subject_path,  \"r\") as hf:\n",
    "             total_num_slices = hf['kspace'].shape[0]\n",
    "\n",
    "        #for train data      \n",
    "        for i in range(len(train_inx)):\n",
    "            if (train_inx[i] == count):\n",
    "                train_data_list['train'][i] = (file_path+ \"/\" +fname, fname, total_num_slices)  \n",
    "                break\n",
    "        #for validation data   \n",
    "        for k in range(len(val_inx)):\n",
    "            if (val_inx[k] == count):\n",
    "                train_data_list['val'][k] = (file_path+ \"/\"+ fname, fname, total_num_slices)\n",
    "                break\n",
    "        count = count +1\n",
    "\n",
    "            \n",
    "    train_data_final ={}         \n",
    "    train_data_final['train'] = []  \n",
    "    train_data_final['val'] = []  \n",
    "    \n",
    "    #The previous function only shuffled the H5files but did not include the slices. The slices are included\n",
    "    #in the correct order here for both train and validation\n",
    "    for i in range (len(train_data_list['train'])):\n",
    "        for slice in range (5,train_data_list['train'][i][2]):\n",
    "            train_data_final['train'].append((train_data_list['train'][i][1], train_data_list['train'][i][0], (slice)))\n",
    "        \n",
    "\n",
    "    for i in range (len(train_data_list['val'])):\n",
    "        for slice in range (5,train_data_list['val'][i][2]):\n",
    "            train_data_final['val'].append((train_data_list['val'][i][1], train_data_list['val'][i][0], slice))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Varification of shuffled data lists for loading\n",
    "print(\"VALIDATION SET\")\n",
    "valcount = 0\n",
    "for i in range (len(train_data_final['val'])):\n",
    "    print(\" count : \",valcount,\"  \", train_data_final['val'][i])\n",
    "    valcount = valcount + 1\n",
    "    \n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "train_count = 0    \n",
    "print(\"TRAIN SET\")\n",
    "for i in range (len(train_data_final['train'])):\n",
    "    print(\" count : \",train_count,\"  \", train_data_final['train'][i])\n",
    "    train_count = train_count + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    data_path_train = '/data/local/NC2019MRI/train'\n",
    "    data_path_val = '/data/local/NC2019MRI/train'\n",
    "    data_list = load_data_path(data_path_train, data_path_val) # first load all file names, paths and slices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads_train_data\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #This is where the undersampling parameters are set. In this ###################\n",
    "    acc = 8\n",
    "    cen_fract = 0.04\n",
    "    seed = False # random masks for each slice \n",
    "    \n",
    "    #More works the better.....############\n",
    "    num_workers = 12 \n",
    "    \n",
    "    \n",
    "    #Train batch size set to 5 optimum\n",
    "    train_dataset = MRIDataset(train_data_final['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=5, num_workers=num_workers) \n",
    "    \n",
    "    # Validation: turned off in final models, used in experimetaly \n",
    "    validation_dataset = MRIDataset(train_data_final['val'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    validation_loader = DataLoader(validation_dataset, shuffle=True, batch_size=5, num_workers=num_workers)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where the neural network model was trained. The parameters and their values present below was that of the best overall results, however is is noteworthy that validation was turned off in final models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set training\n",
    "%matplotlib inline \n",
    "\n",
    "# set learning rate\n",
    "lr = 1e-3\n",
    "\n",
    "#set number of epoches, i.e., number of times we iterate through the training set\n",
    "epoches = 50\n",
    "\n",
    "#We use mean square error (MSELoss) a base parameter.\n",
    "loss_fn = nn.MSELoss(reduction='mean').to(device)\n",
    "\n",
    "# Optimisers a variable parameter that was experimented with\n",
    "optimiser = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "#Lists for plotting val/train SSIM/Loss/prEpoch\n",
    "train_av_epoch_ssim_list = []\n",
    "train_av_epoch_loss_list = []\n",
    "\n",
    "val_av_epoch_ssim_list = []\n",
    "val_av_epoch_loss_list = []\n",
    "\n",
    "epoch_count = 0\n",
    "\n",
    "#Model training\n",
    "for epoch in range(epoches):\n",
    "    \n",
    "    # Set the model to training mode.\n",
    "    model.train()\n",
    "    \n",
    "    #For generating average calculation for each epoch of SSIM and Loss\n",
    "    train_total_loss = 0\n",
    "    train_total_ssim = 0\n",
    "    train_iterations= 0\n",
    "    \n",
    "    val_total_loss = 0\n",
    "    val_total_ssim = 0\n",
    "    val_iterations= 0\n",
    "    \n",
    "    #Training\n",
    "    for data in train_loader:\n",
    "\n",
    "        #The cropped ground truth target and the undersampled input are unloaded\n",
    "        #here for use in generation of our prediction.\n",
    "        img_gt, img_und, _ = data\n",
    "        \n",
    "        #pushed to the GPU\n",
    "        img_gt = img_gt.to(device) \n",
    "        img_und.to(device) \n",
    "        \n",
    "        #zeros the gradient #######\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        #Reconstruction/prediction from undersampled data\n",
    "        y_pred = model(img_und.to(device))\n",
    "        \n",
    "        #In order for to calculate SSIM and to visualise the images,\n",
    "        #the data must be taken back off the GPU, and converted to a numpy array\n",
    "        #from a tensor object.\n",
    "        A = img_und.squeeze().squeeze().numpy()[0:1,:,:]\n",
    "        B = y_pred.squeeze().squeeze().squeeze(0).detach().cpu()\n",
    "        B = y_pred.squeeze().data.cpu().numpy()\n",
    "        B = B[0:1, :, :]\n",
    "        C = img_gt.squeeze().squeeze().squeeze(0).cpu().numpy()[0:1,:,:]\n",
    "        \n",
    "    \n",
    "\n",
    "        #Visualise results. Every 10 iterations all three data set images\n",
    "        #are displayed for visual comparision. This is a useful tool for \n",
    "        #watching the progress of the training, and to identify any potential errors\n",
    "        #at an early stage. This is because a percieved good SSIM value and low loss\n",
    "        #does not neccessarily translate to an accurate image from practice.\n",
    "        \n",
    "        #Concatenates all the data\n",
    "        all_imgs = np.concatenate([A,B,C], axis=0)\n",
    "        \n",
    "        # 0 = UNDERSAMPLED. 1 = PREDICTION. 2 = GROUND_TRUTH\n",
    "        #if train_iterations % 10 == 1:\n",
    "            #show_slices(all_imgs, [0,1,2], cmap='gray')\n",
    "            #plt.pause(1)\n",
    "\n",
    "        \n",
    "        #calculate the loss from training data\n",
    "        loss = loss_fn(img_gt.to(device), y_pred)\n",
    "        loss.backward() \n",
    "        optimiser.step()\n",
    "        \n",
    "        #calculate total loss for epoch to generate average loss\n",
    "        train_total_loss = train_total_loss + loss.item()\n",
    "        \n",
    "        # calculate total ssim value for epoch to generate average ssim\n",
    "        ssim_value = ssim(C,B)\n",
    "        train_total_ssim = train_total_ssim + ssim_value\n",
    "     \n",
    "\n",
    "        #count\n",
    "        train_iterations = train_iterations +1\n",
    "        \n",
    "    #Calculate average train SSIM value and Loss values per epoch    \n",
    "    train_av_ssim = train_total_ssim/train_iterations\n",
    "    train_av_loss = train_total_loss/train_iterations\n",
    "    \n",
    "    train_av_epoch_ssim_list.append(train_av_ssim)\n",
    "    train_av_epoch_loss_list.append(train_av_loss)\n",
    "    \n",
    "    \n",
    "    #Validation: After each epoch the validation is conducted on the model on the validation set. \n",
    "    #In the final models all validation is turned off, and the full training data set was used for reasons \n",
    "    #discussed in experiements section.\n",
    "    \n",
    "    \n",
    "    #Set to no grad, as back propogation is not requried in evalution/validation of model #############\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        #Set to evaluation mode. This is because it is important the model does not\n",
    "        #see this data, as it would invalidate its purpose. Described in report in more detail.\n",
    "        model.eval()\n",
    "        for data in validation_loader:\n",
    "\n",
    "            #from data loader\n",
    "            img_gt, img_und, _ = data\n",
    "            \n",
    "            #Reconstruction/prediction from undersampled data\n",
    "            img_gt = img_gt.to(device) \n",
    "            img_und.to(device) \n",
    "            optimiser.zero_grad()\n",
    "            y_pred = model(img_und.to(device))\n",
    "            \n",
    "            #As in training step\n",
    "            A = img_und.squeeze().squeeze().numpy()[0:1,:,:]\n",
    "            B = y_pred.squeeze().squeeze().squeeze(0).detach().cpu()\n",
    "            B = y_pred.squeeze().data.cpu().numpy()\n",
    "            B = B[0:1, :, :]\n",
    "            C = img_gt.squeeze().squeeze().squeeze(0).cpu().numpy()[0:1,:,:]\n",
    "            \n",
    "            #calculate the loss from validation data\n",
    "            val_loss = loss_fn(img_gt.to(device), y_pred)\n",
    "            optimiser.step()\n",
    "            \n",
    "            #calculate total loss for epoch to generate average loss\n",
    "            val_total_loss = val_total_loss + loss.item()\n",
    "        \n",
    "            #calculate total ssim value for epoch to generate average ssim\n",
    "            ssim_value = ssim(C,B)\n",
    "            val_total_ssim = val_total_ssim + ssim_value\n",
    "            \n",
    "            #count\n",
    "            val_iterations = val_iterations + 1\n",
    "    \n",
    "        #Calculate average validation SSIM value and Loss values per epoch\n",
    "        val_av_ssim = val_total_ssim/val_iterations\n",
    "        val_av_loss = val_total_loss/val_iterations\n",
    "    \n",
    "        #Store in list for graphing\n",
    "        val_av_epoch_ssim_list.append(val_av_ssim)\n",
    "        val_av_epoch_loss_list.append(val_av_loss)\n",
    "    \n",
    "    #For visualizing progress\n",
    "    epoch_count = epoch_count + 1\n",
    "    print(\"epoch_count \", epoch_count)\n",
    "\n",
    "    #Varification of lists\n",
    "print(\"epoch: \", epoch_count,\"train_av_epoch_ssim_list :\", train_av_epoch_ssim_list)\n",
    "print(\"epoch: \", epoch_count,\"train_av_epoch_loss_list :\", train_av_epoch_loss_list)\n",
    "print(\"epoch: \", epoch_count,\"val_av_epoch_ssim_list :\", val_av_epoch_ssim_list)\n",
    "print(\"epoch: \", epoch_count,\"val_av_epoch_loss_list :\", val_av_epoch_loss_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALIDATION VS TRAINING SSIM COMPARISON\n",
    "plt.plot(range(len(train_av_epoch_ssim_list)), train_av_epoch_ssim_list,'r', label = \"Av Train SSIM\")\n",
    "plt.plot(range(len(val_av_epoch_ssim_list)), val_av_epoch_ssim_list,'b', label = \"Av Val SSIM\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Av SSIM\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d9d32bd13fcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#VALIDATION VS TRAINING LOSS COMPARISON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_av_epoch_loss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_av_epoch_loss_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Av Training loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_av_epoch_loss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_av_epoch_loss_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Av Val loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#VALIDATION VS TRAINING LOSS COMPARISON\n",
    "plt.plot(range(len(train_av_epoch_loss_list)), train_av_epoch_loss_list,'r', label = \"Av Training loss\")\n",
    "plt.plot(range(len(val_av_epoch_loss_list)), val_av_epoch_loss_list,'b', label = \"Av Val loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"unet_model_save_50ep_sgd_le1_b5_L1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_av_epoch_ssim_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c5b77b51bdbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#x = val_av_epoch_loss_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#x = train_av_epoch_ssim_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_av_epoch_ssim_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_av_epoch_ssim_list' is not defined"
     ]
    }
   ],
   "source": [
    "#Generates averages for entire model as an approximate measure for\n",
    "\n",
    "#x = train_av_epoch_loss_list\n",
    "#x = val_av_epoch_loss_list\n",
    "#x = train_av_epoch_ssim_list\n",
    "x = val_av_epoch_ssim_list\n",
    "\n",
    "total = 0\n",
    "for i in range(len(x)):\n",
    "    total = total +x[i]\n",
    "    av = total/(len(x))\n",
    "print(av)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Biological Matter possess natural magnetic properties which can be mainipulated using a combination of magnetic disruptions and radio waves to produce KData [1]. \n",
    "\n",
    "KData in itself is not useful for visualising biological features, however when an inverse discrete fourier transform (DFT) is applied to the data, the data can be visualized.\n",
    "KData contains complex numbers, therefore it must be preprocessed before visualization can occur (complex numbers made absolute). KData is held within 3 dimensions and is comprised of voxels. Each voxel contains a partition of the total kdata.\n",
    "\n",
    "A common problem for producing accurate Kdata is the long acquisition time, which converts to higher financial costs and inefficiency in healthcare. By using a low acquisition time, the end result is less accurate. However by applying a neural network model, the image can be accurately predicted [2].\n",
    "\n",
    "In this task, two sets of data were provided, each containing kdata.h5 files. Each H5 file contains multiple slices, which are stacked to form a 3D object.  In the test set, there were 30 low acquisition(low accuracy) H5 files, and in the training set were 70 h5 files each contain high acquisition (high accuracy). \n",
    "The original H5 files of the training set were used as the ground truth, and acted as the control of the experiment and the target output in the neural network designed.Two secondary data sets were created from undersampling the original training data, one 8-fold(high undersampled, and inaccurate), and one 4-fold(less undersampled).\n",
    "\n",
    "The undersampled was created by applying ‘masks’ to the original ground truth data, which essential removed some Kdata information, and in doing so mimicked a low acquisition data MRI scan.\n",
    "The aim of the experiment was to generate 2 neural network models using both of these undersampled sets with original target set, which can accurately convert low acquisition images to a more accurate representation, resembling that of the original ground truth set. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design\n",
    "\n",
    "\n",
    "## Data set\n",
    "\n",
    "Training data is used to train our model and determined the parameters of the model in order to achieve the best results. But during the training process, when the bias of the data set is too high, it will cause the model we trained produce overfitting.  \n",
    "\n",
    "Due to the role of fitting the high-frequency noise elements, large amplitudes appear when higher-order sessions accompany. In this case, the high order polynomial courses will generate the corresponding high-frequency fluctuations required [3].\n",
    "\n",
    "\n",
    "This leads to the large deviations we see between the sampled points. This is known as overfitting which is a constant dilemma when working with the noisy datasets or a complex model. A model that overfits its training data tends to be able to generalise to unseen data and is not helpful in the process of machine learning.\n",
    "\n",
    "To solve this problem, validatation of the data is required to ensure that the model we perceive is the optimal one. In our design, we divide the training data set into 80% training data and 20% validation data. After training the model with training data, we use the validation data set to test all models and then select the model with the least error rate. After formulating the optimal model, we then use the test data set to test the generalized ability of the trained model.\n",
    "\n",
    "## SSIM\n",
    "\n",
    "SSIM is an index used to measure images similarity. With the use of this, we use the images received after training to compare with ground truth. SSIM measures the quality of the results from the aspects of image brightness, contrast and structure [4]. The larger the value is, the closer the result is to ground truth, which means the model is better.\n",
    "\n",
    "\n",
    "## Model¶\n",
    "\n",
    "First and foremost, a literary search was conducted in order to identify the best platform to build upon. Two models were of interest, UNET and GAN.\n",
    "\n",
    "\n",
    "### Gan  {###reduce, condense, much smaller, consise}\n",
    "\n",
    "GAN is a base model for the neural network. It is composed of a generator and a discriminator. The purpose of the generator is to generate fake targets in an attempt to completely deceive the discriminator. The discriminator improves its discrimination ability by learning the true target and the false target, so as not to let the false target fool itself. The two evolved and played against each other. Finally, the evolution stopped until the false target was very similar to the true target. However, it may generate some unpredicted results due to its unsupervised scheme and still get a high score [5].\n",
    "\n",
    "## U-net was another model of interest\n",
    "\n",
    "![unet.png](./unet.png)\n",
    "In 2015, Olaf Ronneberger and others proposed a U-net network structure. The U-net network is a semantic segmentation network based on FCN, which is suitable for medical image segmentation [6].\n",
    "\n",
    "The entire U-Net network structure is shown in the figure above, similar to a large U letter: first convolution + Pooling down-sampling; then deconvolution for up-sampling, low-level feature map before the crop, and fusion; then up-sampling again. This process is repeated until a feature map is generated with an output of 388x388x2, and then finally obtain the output segment map through soft-max. Overall, it is very similar to the FCN idea. Unet's original intention is to solve the problems of biomedical images. This gave rise to it being widely used in various directions of semantic segmentation, such as satellite image segmentation and industrial defect detection [7].\n",
    "\n",
    "\n",
    "### Conclusion \n",
    "###### Advantages of U-net\n",
    "• Support for training models with a small amount of data \n",
    "\n",
    "• Get higher segmentation accuracy by classifying each pixel \n",
    "\n",
    "• Fast segmentation with trained models \n",
    "\n",
    "From the findings from literature, it was decided that UNET offered the best attributes for this specific assignment.Unet has been more established, and has yielded promising results. Furthermore, U-net has a fast processing speed and a higher segmentation accuracy, and has shown to yield good results with little data, which isn this case are valuable attributes.\n",
    "\n",
    "\n",
    "Once unet was decided upon, the parameters were tested in order to identify their optimums, in doing so miniminizing loss values, and a maximizing SSIM values. This was conducted through a combination of literary search, and trial and error\n",
    "\n",
    "\n",
    "## Batch Size \n",
    "\n",
    "It was reasoned an optuimum batch size for the model would be a reasonable starting point, as it would influence the speed of running further tests.\n",
    "\n",
    "Batch size is the number of samples selected for a single training session [8]. Its size affects the degree and speed of optimization of the model. At the same time, it directly affects the GPU memory usage. When chose the best batch size, it can improve memory utilization through parallelization by enabling the GPU to run at full capacity to improve training speed. \n",
    "\n",
    "Also, the number of iterations of a single epoch is reduced and the adjustment of parameters is slow, which more epochs are required to achieve the same recognition accuracy.\n",
    "\n",
    "Most importantly, the optimimum batch size for a particular model will improve the gradient descent.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Optimizer and learning rate\n",
    "\n",
    "The performance of the optimiser is highly dependent on the learning rate provided, and they do not all neccesarily have the same optimum learning rate value. Therefore each optimisation function was tested with and array of learning  rate values from 1e^-1 to 1e^-4.\n",
    "\n",
    "\n",
    "# Optimizers tested\n",
    "The main purpose of training a neural network is to find suitable parameters and obtain minimal loss values. The process of solving this problem is called optimization. The algorithm used to solve this problem is called optimizer. Among various optimizers, we choose three of them as listed below into our account. These three are basicly improved versions based on other optimizers, thus can achieve better result.  \n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Stochastic gradient descent (SGD) computes the gradient of the cost function with respect to the parameters $\\theta$. What's more, the whole training dataset delivers a parameter update for each training example $x^{(i)}$ and label $y^{(i)}$. It prevents redundant computations for large datasets and usually processes relatively fast. However, since SGD conducts frequent updates with high variance, it can cause the objective function to fluctuate rigorously. Also, when facing local optimal where curves are much more steeply in one dimension than in another. SGD oscillates across the area and makes slow progress [9]. This leads to RMSprop.  \n",
    "$$\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J ( \\theta ; x^{(i)} ; y^{(i)} ) $$\n",
    "\n",
    "### RMSprop\n",
    "\n",
    "For the learning rate of models, traditional optimizer either set the learning rate to a constant or adjust the learning rate based training results. All ignore the potential of the learning rate.\n",
    "\n",
    "We can consider RMSprop as an adaptation of the rprop algorithm to learning with mini-batch. This was the original motivation for developing the algorithm. It performs low learning rates on parameters related to frequently occurring features and performs high learning rates to obtain parameters related to less commonly used features [10]. Therefore, it is very suitable for dealing with sparse data. In this way, it can greatly improve the robustness of SGD and solve the problem of the sharp drop in learning rate. It is assumed that during the $t$ iteration, each formula is as follows:\n",
    "\n",
    "\n",
    "$${\n",
    "        s_{dw}= \\beta s_{dw} + (1 - \\beta)d W^2 \\\\\n",
    "        s_{db}= \\beta s_{db} + (1 - \\beta)d b^2 \\\\\n",
    "        W = W - \\alpha \\frac{dW}{\\sqrt{s_{dw}} + \\varepsilon} \\\\\n",
    "        b = b - \\alpha \\frac{db}{\\sqrt{s_{dw}} + \\varepsilon}\n",
    " }$$\n",
    "\n",
    "\n",
    "In the above formula, $s_{dw}$ and $s_{db}$ are the gradient momentum accumulated by the loss function during the first $t-1$ iterations,.Respectively, $\\beta$ is an index of gradient accumulation. In order to prevent the denominator from being zero, a very small value $\\varepsilon$ is used for smoothing which generally take 1e-8.\n",
    "\n",
    "The RMSprop algorithm calculates the differential squared weighted average of the gradient. This method is beneficial to eliminate the direction of large swing amplitude and is used to correct the swing amplitude so that the swing amplitude in each dimension is smaller. On the other hand, it also makes the network function converge faster.\n",
    "\n",
    "\n",
    "### Adaptive Moment Estimation (Adam)\n",
    "\n",
    "Adaptive Moment Estimation (Adam) is a method that computes adaptive learning rates for each parameter [11]. In addition to collecting past squared gradients' exponentially decaying average $v_t$, Adam also keeps an exponentially decaying average of past gradients $m_t$ which similar to momentum. In this way, it can flat minima in the error surface. We compute the $v_t$ and $m_t$ as follows: \n",
    "\n",
    "$${\n",
    "     m_t = \\beta_1 m_{t-1} + ( 1- \\beta_1) g_t \\\\\n",
    "     v_t = \\beta_2 v_{t-1} + ( 1- \\beta_2) g_t^2\n",
    " }$$\n",
    " \n",
    "If $m_t$ and $v_t$ are initialized as zero vectors, they will be biased towards 0, so a bias correction appear. It can offset these deviations by calculating the updated $m_t$ and $v_t$ as follows:\n",
    "\n",
    "$${\n",
    "     \\hat m_t = \\frac{m_t}{1 - \\beta_1^t} \\\\\n",
    "     \\hat v_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    " }$$\n",
    "\n",
    "and last upated Adam is:\n",
    "\n",
    "$${\n",
    "        \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat v_t} + \\varepsilon} \\hat m_t\n",
    " }$$\n",
    " \n",
    "\n",
    "where $\\beta_1$ and $\\beta_2$ is often to set close to $1$\n",
    "\n",
    "Adam is generally considered to be quite robust in the choice of hyperparameters, although the learning rate sometimes needs to be modified to default.\n",
    "\n",
    "## Learning Rates tested\n",
    "\n",
    "The learning rate determines how far the weights move in the gradient direction in a batch. Choosing a suitable learning rate requires constant experimentation and adjustment.\n",
    "\n",
    "When the learning rate is very small, the training will become reliable, that is, the gradient will gradually approach the minimum value. The calculated loss value will become smaller as well as the cost is that the training process will take longer. When the learning rate is very large, the training process will ignore the minimum value, showing that the loss value continuously fluctuates. In the most severe cases, it may never reach the minimum value, or even jump out of this range and fall into another sinking area [12].\n",
    "\n",
    "The learning rate determines how far the weights move in the gradient direction in a batch. Choosing a suitable learning rate requires constant experimentation and adjustment.\n",
    "\n",
    "\n",
    "Therefore from intailly identifying the optimum batch size, the most accurate models were identified by comparing both SSIM and loss values in the training modules for both 8f and 4f.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Loss Function\n",
    "\n",
    " The predicted value obtained after using the model for training in the neural network needs to be compared with the ground truth so that the model can self-corrects until the difference becomes really small. With different loss functions, the influence on the model will differ too. We need to choose the most suitable function to get the most ideal model. We decided to use L1 as the orimary loss functions as: \n",
    "\n",
    "### L1 Loss\n",
    "\n",
    "$${\n",
    "  loss(x,y)= \\sum_{i=1}^{n} (x-y)^2 \n",
    " }$$\n",
    "\n",
    "L1 Loss Function is used to minimize the error which is the sum of the all the absolute differences between the true value $x$ and the predicted value $y$. This Function is not affected by the outliers or remove the outliers [13].\n",
    "\n",
    "Testing multiple loss functions would have also been a valid way to find optimum parameters to produce an accurate model. However with the time constraints is was reasoned only L1 MSE loss would be used a platform. L1 MSE loss was chosen, due to its frequent use in literature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch\n",
    "\n",
    "We calculated the average loss value and SSIM of both training data and validation data set with the batch from 3 to 6 when epochs is 0 to 50. We can conclude from the table that the SSIM and is better when the Batch is 5 as we list the result as below.\n",
    "\n",
    "|    Batch Size    | Average Training Data Loss Value  |    Average Validation Data Loss Value   | Average Training Data SSIM |    Average Validation Data SSIM    |\n",
    "| ---------- | --- | --- | --- | --- |\n",
    "| 3 |  0.00816| 0.00859| 0.47535 | 0.50365|\n",
    "| 4 |  0.00808| 0.00784| 0.48208 | 0.45985|~\n",
    "| 5 |  0.00891| 0.00913| <b>0.48697<b> | <b>0.53715<b>|\n",
    "| 6 |  0.00872| 0.00890| 0.46395 | 0.49930|\n",
    "\n",
    "When the batch is 5, the obtained data curve is shown below. Both the average SSIM and the SSIM at 50 epochs of the training data set is around 0.46. Since the SSIM of validation data set is more vibrate and with an average of 0.53 but can reach about 0.57 at 50 epochs, which is significant. \n",
    "\n",
    "In finding these values it was found that the fluctuations for the noise for Loss was very high, where the train value for loss was very stable. This was the same trend for all experiments for batch size. It was then decided that validation was to be turned off for further experiments. This was becuase there did not seem to show signs of over fitting with SSIM valuesa ndthe results were too noisey to make valid conclusions from findings. Additionally,the data set was intially small, and it was therefore reasoned that the extra data may have proved more useful inside the model. In doing so all further experiments was capped at 50 epochs to further ensure that overfitting risk was limited. \n",
    "As an additional experiment these parameters were run on 120 epochs where despite a high ssim value of approximately 0.6, the end result image was not clear, indicating overfitting. \n",
    "\n",
    "\n",
    "![Batch5Loss.png](./Batch5Loss.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The comparsion of optimizers\n",
    "\n",
    "The following table indicates the results of the three optmizers we analysis in both 4 and 8 times acceleration rate with different learning rate. The vital results are shown in bold.\n",
    "\n",
    "|   Optimizer  |  Learning Rate  |  AV_SSIM_4F  |  Final_SSIM_4F  |  AV_SSIM_8F  |  Final_SSIM_8F  |  AV_LOSS_4F  |  AV_LOSS_8F  |\n",
    "| ---------- | --- | --- |---------- | --- | --- | --- | --- |\n",
    "|ADAM |1.00E-01 |0.4047 |n/a |0.3467 |0.352|0.08962 |0.05603 |\n",
    "|ADAM |1.00E-02 |0.5406 |0.54  |0.4368 |0.449| 0.00605|0.01142|\n",
    "|ADAM |1.00E-03 |0.5869 |0.60 |0.4892 |0.501 |0.00439 |0.00828|\n",
    "|ADAM |1.00E-04 |<b>0.5898<b> |<b>0.61<b> |<b>0.5006<b> |<b>0.525<b> |<b>0.00436 <b>|<b>0.00769<b>|\n",
    "|RMS |1.00E-01 |0.2305 |NA |0.2235 |0.270 |28.2024 |12.4788 |\n",
    "|RMS |1.00E-02 |0.5666 |0.60 |0.4892 |0.521 |0.00537 |0.86531 |\n",
    "|RMS |1.00E-03 |0.5936 |0.62 |0.5048 |0.535 |0.00415 |0.00738 |\n",
    "|RMS |1.00E-04 |<b>0.5942 <b>|<b>0.625 <b>|<b>0.5047<b> |0.529 |<b>0.00408 <b>|0.0073 |\n",
    "|SGD | 1.00E-01 |<b>0.5746 <b>|<b>0.6 <b>|<b>0.5047 <b>|<b>0.495<b> |<b>0.00425<b> |<b>0.00825<b> |\n",
    "|SGD |1.00E-02 |0.5235 |0.56 |0.4369 |0.472 |0.00579 |0.01049 |\n",
    "|SGD |1.00E-03 |0.4194 |0.46 |0.3557 |0.398 |0.01064 |0.0167 |\n",
    "|SGD |1.00E-04 |0.2391 |0.31 |0.2271 |0.301 |0.02271 |0.30992|\n",
    "\n",
    "### ADAM \n",
    "\n",
    "For 4-F, the best SSIM value was marginally the best with a learning rate of 1e-4 0.5898, with a final SSIM value presenting a value of average at 0.59  F0.61, with 1e-3 being a close second. The average loss value was also marginally lower than le-3, and reinforces the validity of the SSIM final reading.\n",
    "  \n",
    "8-F also followed suit with 4-fold mirroring trends, and further verifying findings. With highest SSIM value average 0.5 F0.525. The loss values were also the lowest with values of 0.0077.\n",
    "\n",
    "For both 8F and 4F, when learning rate is 1e-1m, showed to be significantly worse across both SSIM and loss values. Furthermore it resulted in abnormal and unstable. The graphs suggesting that 1e-1 was much too large for the optimum to be found in the usual logarithmic trend of other parameters. \n",
    "\n",
    "![SSIM1.png](./SSIM1.png)\n",
    "![Loss1.png](./Loss1.png)\n",
    "\n",
    "\n",
    "The graphs below is the comparison of average SSIM as well as the average loss value in 1e-4 and 1e-3. It further supports the findings of ADAM providing the best results. Where the SSIM values appear more stable in 1e-4 showing less fluctuation. Furthermore, 1e-4 reached its point of diminished returns much earlier than for 1e-3. The is further supported in the visualisation of the images as shown in images index, where l-4 shows slightly more detail. \n",
    "\n",
    "![LR44fSSIM.png](./LR44fSSIM.png)\n",
    "![LR34fSSIM.png](./LR34fSSIM.png)\n",
    "![LR44fLoss.png](./LR34fLoss.png)\n",
    "![LR34fLoss.png](./LR34fLoss.png)\n",
    "\n",
    "\n",
    "### RMSprop\n",
    "\n",
    "The result of RMSprop was not dissimilar to the results of of ADAM. Also resulting is very similar SSIM and loss values with learning rate in 1e-4 and 1e-3 being almost identical in the table, and too insignificant to make a conclusion on the best learning rate parameter for RMSprop, both results' average SSIM Inspection of the graphs were also too similar to draw conclusions. Despite this, from visualisation of the images it appeared that 1e-3 gave slightly more detail to the image, seen by eye. Additionally, as with ADAM, the worst value shown were when learning rate was 1e-1, which indicated a too large learning rate for RMSprop to function. This trend was mirrored in 8f also, however with slightly lower results. \n",
    "\n",
    "![SSIM2.png](./SSIM2.png)\n",
    "![Loss2.png](./Loss2.png)\n",
    "\n",
    "### SGD\n",
    "\n",
    "As for SGD, the results of both 8F and 4F appeared to be the inverse of that of RMSprop and Adam. This displayed 1e-1 as the best result with average SSIM in 8F result in 0.57 and 0.6. Loss values were also the lowest with both 8F and 4F. Despite this, the graphs appeared to follow the normal logarithmic trends, however diminished early for Le-4 despite having the lowested value. Unlike RMSprop and Adam's worst values showing much instabilty. This trend was mirrored in 8f also, however with slightly lower results. The images we obtained also show that 1e--4 is the worst and 1e--1 is the most ideal.\n",
    "\n",
    "![SSIM3.png](./SSIM3.png)\n",
    "![Loss3.png](./Loss3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In conclusion, it was identified the optimum batch size was the 5. RMSprop had LR 1e-3 and LR 1e-4  as the highest values for SSIM, however not the lowest values for loss. Upon inspection of the images it appeared that RMSprop 1e-3  when comparing all images was the most detailed, and therefore was the model selected for our final reconstruction. Results are shown in the Google drive link at the top of the report. Further research may have included experients with more loss functions. Another experiment that maay have been useful for further experimentation would have been to try IBM quantum computing server to calculate a true random array for shuffling value to identify if generalization to test data would have increased.\n",
    "\n",
    "\n",
    "IMAGE COMPARISONS labelled are also shown in Google Drive which is refered to in report, when images are discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contribution\n",
    "\n",
    "\n",
    "joe maliszewski = 40%\n",
    "Jin Zhi = 25%\n",
    "Yip Sen Man = 25%\n",
    "Rui Ding = 10%\n",
    "Liming Liu = 0%\n",
    "Haotian Zhang = 0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "[1] Abi B, Magnetic resonance imaging. The BMJ, 2012\n",
    "\n",
    "[2] Weiming L, Tong T, Qinquan G, Convolutional Neural Networks-Based MRI Image Analysis for the Alzheimer’s Disease Prediction From Mild Cognitive Impairment, Frontiers in Neuroscience, 2018\n",
    "\n",
    "[3] Nitish S, Geoffrey E.H, Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 2014\n",
    "\n",
    "[4] Alain H, Djemel Z, Image Quality Metrics: PSNR vs. SSIM. International Conference on Pattern Recognition, 2010\n",
    "\n",
    "[5] Sebastian N, Botond C, Ryota T, f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization.  Advances in Neural Information Processing Systems, 2016 \n",
    "\n",
    "[6] Ronneberger O, Fischer P, Brox T, U-net: Convolutional networks for biomedical image segmentation. International Conference on Medical image computing and computer-assisted intervention, 2015\n",
    "\n",
    "[7] Wenjun Y, Yuanyuan W, Shengjia G, The Domain Shift Problem of Medical Image Segmentation and Vendor-Adaptation by Unet-GAN. International Conference on Medical Image Computing and Computer-Assisted Intervention, 2019 \n",
    "\n",
    "[8] Alfredo C, Adam P, Eugenio C, An Analysis of Deep Neural Network Models for Practical Applications. arXiv preprint arXiv:1605.07678, 2016\n",
    "\n",
    "[9] Bottou L, Large-Scale Machine Learning with Stochastic Gradient Descent. Proceedings of COMPSTAT, 2010\n",
    "\n",
    "[10] Geoffrey Hinton Neural Networks for machine learning nline course. https://www.coursera.org/learn/neural-networks/home/welcome\n",
    "\n",
    "[11] Bottou, L, The Tradeoffs of Large Scale Learning.  Optimization for Machine Learning, 2012\n",
    "\n",
    "[12] Robert A.Jacobs Increased rates of convergence through learning rate adaptation. Neural Networks, 1988\n",
    "\n",
    "[13] Jonathan T. Barron, A General and Adaptive Robust Loss Function. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
